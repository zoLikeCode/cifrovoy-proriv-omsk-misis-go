{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "2-RfG_rjHKPm"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download(\"stopwords\")\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "!pip install stop_words\n",
        "import stop_words\n",
        "import re"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JTtX8bCGHS8-",
        "outputId": "4260393d-f3eb-47bf-d732-aa15f9399f3c"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: stop_words in /usr/local/lib/python3.10/dist-packages (2018.7.23)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade scikit-learn==1.4.2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EHzGmS64HsvD",
        "outputId": "822b38d7-1727-4d8f-f2cc-6ea92b7c41d1"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn==1.4.2 in /usr/local/lib/python3.10/dist-packages (1.4.2)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.4.2) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.4.2) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.4.2) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.4.2) (3.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import mean_squared_error"
      ],
      "metadata": {
        "id": "umRCiUdlWtsy"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_data = pd.read_csv('/content/TRAIN_SAL.csv', encoding='utf-8')\n",
        "raw_data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_BeRT2uHbjP",
        "outputId": "862a8187-48f1-482d-8994-915c1b503b3d"
      },
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-148-e0cad78b9a5e>:1: DtypeWarning: Columns (1,16,24,69) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  raw_data = pd.read_csv('/content/TRAIN_SAL.csv', encoding='utf-8')\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(631117, 78)"
            ]
          },
          "metadata": {},
          "execution_count": 148
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raw_data = raw_data.sample(frac=1).reset_index(drop=True)\n"
      ],
      "metadata": {
        "id": "a7e9n3GZZ1iv"
      },
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocessing_salary_prediction(raw_data):\n",
        "  #отбор наиболее релевантных колонок\n",
        "  features_name = ['id', 'required_experience', 'vacancy_address_latitude', 'vacancy_address_longitude', 'professionalSphereName', 'position_requirements', 'salary']\n",
        "\n",
        "  raw_data = raw_data[features_name]\n",
        "\n",
        "  #удаление полей с отсутствующей зарплатой\n",
        "  raw_data['salary'] = raw_data['salary'].replace({0:np.nan})\n",
        "  raw_data = raw_data.dropna()\n",
        "\n",
        "  vacancies_copy = raw_data.copy()\n",
        "\n",
        "  # NLP препроцессинг поля position_requirements\n",
        "  def remove_punctuation(text):\n",
        "    return ''.join([ch if ch not in string.punctuation else ' ' for ch in text])\n",
        "  def remove_numbers(text):\n",
        "    return ''.join([i if not i.isdigit() else ' ' for i in text])\n",
        "  def remove_eng(text):\n",
        "    return ''.join([' ' if i[0] in ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z'] else i for i in text])\n",
        "  def remove_multiple_spaces(text):\n",
        "    return re.sub(r'\\s+', ' ', text, flags=re.I)\n",
        "\n",
        "  prep_text = [remove_multiple_spaces(remove_eng(remove_numbers(remove_punctuation(text.lower())))) for text in vacancies_copy['position_requirements'].astype('str')]\n",
        "\n",
        "\n",
        "  # Кодируем position_requirements c помощью преобразования TF-IDF\n",
        "  russian_stopwords = stop_words.get_stop_words('ru')\n",
        "  tfidf = TfidfVectorizer(max_features=5, stop_words=russian_stopwords)\n",
        "  skills_encoded = tfidf.fit_transform(prep_text)\n",
        "  skills_encoded = pd.DataFrame(skills_encoded.toarray(), columns=tfidf.get_feature_names_out()).iloc[:, 1:]\n",
        "\n",
        "  vacancies_copy = pd.concat([vacancies_copy, skills_encoded], axis=1)\n",
        "  vacancies_copy.drop('position_requirements', axis=1, inplace=True)\n",
        "  vacancies_copy = vacancies_copy.dropna()\n",
        "\n",
        "  # Стандартизируем числовые признаки\n",
        "  scaler = StandardScaler()\n",
        "  vacancies_copy[['required_experience', 'vacancy_address_latitude', 'vacancy_address_longitude']] = scaler.fit_transform(vacancies_copy[['required_experience', 'vacancy_address_latitude', 'vacancy_address_longitude']])\n",
        "\n",
        "  '''Удаление выбросов'''\n",
        "  # Колонки, в которых ищем выбросы\n",
        "  cols = ['vacancy_address_latitude', 'vacancy_address_longitude', 'salary']\n",
        "  # Считаем квантили и межквартильный диапазон\n",
        "  Q1 = vacancies_copy[cols].quantile(0.25)\n",
        "  Q3 = vacancies_copy[cols].quantile(0.75)\n",
        "  IQR = Q3 - Q1\n",
        "  # Возвращает логический массив строк со значением признака в допустимых пределах\n",
        "  condition = ~((vacancies_copy[cols] < (Q1 - 1.5 * IQR)) | (vacancies_copy[cols] > (Q3 + 1.5 * IQR))).any(axis=1)\n",
        "  # Отбираем строки соответствующие условию\n",
        "  vacancies_non_outs = vacancies_copy[condition]\n",
        "\n",
        "  final = vacancies_non_outs.copy()\n",
        "  # One-hot encoding поля professionalSphereName\n",
        "  final = pd.get_dummies(final, columns=['professionalSphereName'])\n",
        "  final = final.drop(['professionalSphereName_Маркетинг, реклама, PR'], axis=1)\n",
        "\n",
        "  return final\n"
      ],
      "metadata": {
        "id": "-NBxjZmRHs8K"
      },
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vacancies_prepared = preprocessing_salary_prediction(raw_data)\n",
        "vacancies_prepared = vacancies_prepared.drop('id', axis=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3cslvEANX3sJ",
        "outputId": "ebcc8e45-9c92-40da-bc83-172808f2e4ee"
      },
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-150-0aff5b22ec4b>:8: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  raw_data['salary'] = raw_data['salary'].replace({0:np.nan})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n = 200000\n",
        "y = vacancies_prepared['salary'][:n]\n",
        "X = vacancies_prepared.drop('salary', axis=1)[:n]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
      ],
      "metadata": {
        "id": "F5pHCGRdYCVw"
      },
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2Z6bdNLkzvNU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "estimator = RandomForestRegressor()\n",
        "\n",
        "parameters = {}\n",
        "\n",
        "estimator.fit(X_train, y_train)\n",
        "\n",
        "best_estimator = estimator"
      ],
      "metadata": {
        "id": "AIxI-YfGZxtt"
      },
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocessing_salary_prediction_test(raw_data):\n",
        "  #отбор наиболее релевантных колонок\n",
        "  features_name = ['id', 'required_experience', 'vacancy_address_latitude', 'vacancy_address_longitude', 'professionalSphereName', 'position_requirements']\n",
        "\n",
        "  raw_data = raw_data[features_name]\n",
        "  vacancies_copy = raw_data.copy()\n",
        "\n",
        "  # NLP препроцессинг поля position_requirements\n",
        "  def remove_punctuation(text):\n",
        "    return ''.join([ch if ch not in string.punctuation else ' ' for ch in text])\n",
        "  def remove_numbers(text):\n",
        "    return ''.join([i if not i.isdigit() else ' ' for i in text])\n",
        "  def remove_eng(text):\n",
        "    return ''.join([' ' if i[0] in ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z'] else i for i in text])\n",
        "  def remove_multiple_spaces(text):\n",
        "    return re.sub(r'\\s+', ' ', text, flags=re.I)\n",
        "\n",
        "  prep_text = [remove_multiple_spaces(remove_eng(remove_numbers(remove_punctuation(text.lower())))) for text in vacancies_copy['position_requirements'].astype('str')]\n",
        "\n",
        "\n",
        "  # Кодируем position_requirements c помощью преобразования TF-IDF\n",
        "  russian_stopwords = stop_words.get_stop_words('ru')\n",
        "  tfidf = TfidfVectorizer(max_features=5, stop_words=russian_stopwords)\n",
        "  skills_encoded = tfidf.fit_transform(prep_text)\n",
        "  skills_encoded = pd.DataFrame(skills_encoded.toarray(), columns=tfidf.get_feature_names_out()).iloc[:, 1:]\n",
        "\n",
        "  vacancies_copy = pd.concat([vacancies_copy, skills_encoded], axis=1)\n",
        "  vacancies_copy.drop('position_requirements', axis=1, inplace=True)\n",
        "\n",
        "  # Стандартизируем числовые признаки\n",
        "  scaler = StandardScaler()\n",
        "  vacancies_copy[['required_experience', 'vacancy_address_latitude', 'vacancy_address_longitude']] = scaler.fit_transform(vacancies_copy[['required_experience', 'vacancy_address_latitude', 'vacancy_address_longitude']])\n",
        "\n",
        "  final = vacancies_copy.copy()\n",
        "  # One-hot encoding поля professionalSphereName\n",
        "  final = pd.get_dummies(final, columns=['professionalSphereName'])\n",
        "  final = final.drop(['professionalSphereName_Маркетинг, реклама, PR'], axis=1)\n",
        "\n",
        "  return final"
      ],
      "metadata": {
        "id": "M63qNOJcpqWa"
      },
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = pd.read_csv('TEST_SAL.csv')\n",
        "\n",
        "test_pr = preprocessing_salary_prediction_test(test)\n",
        "\n",
        "test_pr = test_pr.drop(['id'], axis=1)\n",
        "\n",
        "y_pred_t = best_estimator.predict(test_pr)\n"
      ],
      "metadata": {
        "id": "C4oEFhPSpcTU"
      },
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "y_pred = best_estimator.predict(X_test)\n",
        "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
        "\n",
        "print(str(estimator) + \" RMSE: %.2f\" % rmse)\n",
        "custom_metric = max(0,1-(rmse/33000))\n",
        "print(\"custom_metric: %.2f\" % custom_metric)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4nOQMtz5bST7",
        "outputId": "9d6a74ea-f639-4bb8-f5b8-31ec7c14e401"
      },
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RandomForestRegressor() RMSE: 7058.03\n",
            "custom_metric: 0.79\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "filename = 'salary_prediction_model_0.joblib'\n",
        "joblib.dump(best_estimator, filename)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nLKD8fFei7nx",
        "outputId": "f5fcf10a-62e9-4362-fe1c-0a40bdaa9a6a"
      },
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['salary_prediction_model_0.joblib']"
            ]
          },
          "metadata": {},
          "execution_count": 158
        }
      ]
    }
  ]
}